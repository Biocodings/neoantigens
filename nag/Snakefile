import glob
import os
from os.path import join, abspath, dirname, isfile, basename, splitext
import csv

from ngs_utils.vcf_utils import get_tumor_sample_name, vcf_contains_field
from ngs_utils.bcbio import BcbioProject
from ngs_utils.logger import critical, info, debug, warn, err
from ngs_utils.file_utils import verify_file, verify_dir
from reference_data import api as refdata
from nag import balachandran
from nag import nag_summarize

shell.executable(os.environ.get('SHELL', 'bash'))
shell.prefix("")


if config.get('genomes_dir'):
    refdata.find_genomes_dir(config.get('genomes_dir'))
print('Genomes dir:', refdata.genomes_dir)


inputs_by_sname = dict()
if config.get('input_tsv'):
    include_samples = None
    if 'include' in config:
        include_samples = config['include'].split(',')
    exclude_samples = None
    if 'exclude' in config:
        exclude_samples = config['exclude'].split(',')

    with open(config['input_tsv']) as f:
        csv_reader = csv.DictReader(f, delimiter='\t')
        for e in csv_reader:
            if include_samples and e['sample'] not in include_samples:
                print(f"excluding {e['sample']} because not in {include_samples}")
                continue
            if exclude_samples and e['sample'] in exclude_samples:
                print(f"excluding {e['sample']} because in {exclude_samples}")
                continue
            print(f"using {e['sample']}")
            inputs_by_sname[e['sample']] = e
else:
    inputs_by_sname[config['sample']] = dict(
        sample=config['sample'],
        somatic_vcf=config['somatic_vcf'],
        rna_bcbio=config['rna_bcbio'],
        rna_sample=config['rna_sample'],
        optitype_file=config['optitype_file']
    )
for sn, inp in inputs_by_sname.items():
    inp['rna_bcbio'] = verify_dir(inp['rna_bcbio'], is_critical=True, description=f'rna_bcbio for {sn}')
    inp['optitype_file'] = verify_file(inp['optitype_file'], is_critical=True, description=f'optitype_file for {sn}')
    inp['somatic_vcf'] = verify_file(inp['somatic_vcf'], is_critical=True, description=f'somatic_vcf file for {sn}')

GENOME                 = 'hg38'
ENSEMBL_GENOME         = 'GRCh38'
PIZZLY_ENSEMBL_RELEASE = config.get('ensembl_release', 95)
VEP_ENSEMBL_RELEASE    = config.get('ensembl_release', 98)
PYENSEMBL_DIR          = config.get('ensembl_dir', refdata.get_ref_file(genome=GENOME, key='pyensembl_data'))
VEP_DATA               = glob.glob(join(refdata.get_ref_file(genome=GENOME, key='pcgr_data'), '*/.vep'))[0]
IEDB_DIR               = config.get('iedb_dir', refdata.get_ref_file(key='iedb_dir'))
REF_FA                 = config.get('ref_fa', refdata.get_ref_file(genome=GENOME, key='fa'))
USE_REMOTE_DB          = config.get('internet_is_on', False)
PIZZLY_REF_FA          = config.get('pizzly_ref_fa')


# pVACtools parameters
EPITOPE_LENGTHS = '9'  #'8,9,10,11'
PREDICTORS_MHCI = [
    'NetMHCcons',  # NetMHCcons is ensemble of NetMHC,NetMHCpan,PickPocket
    'MHCflurry',
    'MHCnuggetsI',
    'SMM',
    'SMMPMBEC',
]
PREDICTORS_MHCII = [
    'NetMHCIIpan',
    'MHCnuggetsII',
    'NNalign',
    'SMMalign'
]

# pVACfuse parameters
PEP_5_PRIME_FLANK = 14  # Aminoacids to pick from both sides of the fusion junction peptide.
    # It should be long enough so your longest desigred epitope just covers the junction. Say,
    # if the longest epitope length is 11, use PEP_5_PRIME_FLANK of 10. Since MHC_II algorithms
    # always call epitopes of the length 15, we set PEP_5_PRIME_FLANK to 14.

# pVACseq parameters
MIN_NORMAL_COV = 5
MIN_TUMOR_COV = MIN_RNA_COV = 10
MAX_NORMAL_VAF = 2
MIN_TUMOR_VAF = MIN_RNA_VAF = config.get('min_tvaf', 10)
MIN_EXPN = 1  # TPM expression should be at least 1
EXCLUDE_NA = False


rule all:
    input:
        expand('MHC_Class_{cls}_summary_{source}.tsv', cls=['I', 'II'], source=['seq', 'fuse']),
        expand('{sample}/pvacseq_results/MHC_Class_{cls}/{sample}.filtered.condensed.ranked.tsv',
               cls=['I', 'II'], sample=inputs_by_sname.keys()),
        expand('{sample}/pvacfuse_results/MHC_Class_{cls}/{sample}.filtered.condensed.ranked.tsv',
               cls=['I', 'II'], sample=inputs_by_sname.keys()),

rule pvacseq:
    input:
        expand('MHC_Class_{cls}_summary_{source}.tsv', cls=['I', 'II'], source=['seq', 'fuse']),
        expand('{sample}/pvacseq_results/MHC_Class_{cls}/{sample}.filtered.condensed.ranked.tsv',
               cls=['I', 'II'], sample=inputs_by_sname.keys()),

rule pvacfuse:
    input:
        expand('MHC_Class_{cls}_summary_{source}.tsv', cls=['I', 'II'], source=['seq', 'fuse']),
        expand('{sample}/pvacfuse_results/MHC_Class_{cls}/{sample}.filtered.condensed.ranked.tsv',
               cls=['I', 'II'], sample=inputs_by_sname.keys()),


def _pvac_cmdl(tool, input, sample, hla_types, output_dir, threads, other_params=''):
    cmd = (
        f'{tool} run '
        f'{input} '
        f'{sample} '
        f'"$(cat {hla_types})" '
        f'{" ".join(PREDICTORS_MHCI + PREDICTORS_MHCII)} '
        f'{output_dir} '
        f'-e {EPITOPE_LENGTHS} '
        f'--top-score-metric=lowest '
        f'--n-threads {threads} '
        f'--iedb-install-directory {IEDB_DIR} '
        f'--keep-tmp-files '
        f'{other_params} '
    )
    if USE_REMOTE_DB:
        cmd += \
            f' --net-chop-method cterm ' \
            f'--netmhc-stab'
    if EXCLUDE_NA:
        cmd += f' --exclude-NAs'
    return cmd

# rule pvacseq_filter:
#     input:  dynamic('pvacseq_results/MHC_Class_{cls}/{SNAME}.final.tsv'),
#     output: dynamic('pvacseq_results/MHC_Class_{cls}/{SNAME}.final.filt.tsv')
#     shell : 'pvacseq top_score_filter -m median MHC_Class_II/diploid_tumor.final.tsv MHC_Class_II/diploid_tumor.final.TOP.tsv'
    # also filer to remove epitopes shorter than FLANKING_PEPTIDE+1 that start earlier than FLANKING_PEPTIDE+1-len(epitope)
    # in output: len(epitope) = Peptide Length
    #            start        = Sub-peptide Position
    # explore why MHC II takes 31aa


################
### HLA types

# t_optitype = join(batch.tumor.dirpath, batch.tumor.name + '-hla-optitype.csv')

rule prep_hla:
    """ Create a file containing a single line of comma-separated HLA alleles
    """
    input:
         lambda wc: inputs_by_sname[wc.sample]['optitype_file']
    output:
        'work/{sample}/hla_line.txt'
    shell: r"""
    grep -v ^sample {input} | tr ',' '\t' | cut -f3 | tr ';' '\n' | sort -u | tr '\n' ',' | head -c -1 > {output}
    """


################
### Small mutations (for pVACseq)

rule vcf_af_filter:  # {batch}
    input:
         lambda wc: inputs_by_sname[wc.sample]['somatic_vcf']
    output:
         'work/{sample}/vcf_prep/somatic.AF_FILT.vcf.gz'
    params:
         tvaf = MIN_TUMOR_VAF / 100,
         nvaf = MAX_NORMAL_VAF / 100,
         tcov = MIN_TUMOR_COV,
         ncov = MIN_NORMAL_COV,
    shell:  """
    bcftools filter -i "\
    TUMOR_AF>={params.tvaf} && 
    NORMAL_AF<={params.nvaf} && 
    TUMOR_DP>={params.tcov} && \
    NORMAL_DP>={params.ncov}\
    " {input} -Oz -o {output} && tabix {output}
    """

rule drop_older_csq:
    input:
        vcf = 'work/{sample}/vcf_prep/somatic.AF_FILT.vcf.gz',
    output:
        vcf = 'work/{sample}/vcf_prep/somatic.AF_FILT.CLEAN_PREV_VEP.vcf.gz',
    run:
        cmd = f'cat {input.vcf}'
        # remove previous VEP annotation:
        filts_to_remove = [f'{f}' for f in ['INFO/CSQ'] if vcf_contains_field(input.vcf, f)]
        if filts_to_remove:
            cmd += f' | bcftools annotate -x "' + ','.join(f'{f}' for f in filts_to_remove) + '"'
        cmd += ' | bcftools view -f.,PASS -Oz -o {output.vcf} && tabix -p vcf {output.vcf}'
        shell(cmd)

# The --symbol option will include gene symbol in the annotation.
# The --terms SO option will result in Sequence Ontology terms being used for the consequences.
# The --tsl option adds transcript support level information to the annotation.
# The --hgvs option will result in HGVS identifiers being added to the annotation.
#   Requires the usage of the --fasta argument.
# The --cache option will result in the VEP cache being used for annotation.
# The --plugin Downstream option will run the Downstream plugin which will compute the
#   downstream protein sequence after a frameshift.
# The --plugin Wildtype option will run the Wildtype plugin which will include the transcript
#   protein sequence in the annotation.
rule vcf_vep:
    input:
        'work/{sample}/vcf_prep/somatic.AF_FILT.CLEAN_PREV_VEP.vcf.gz',
        vep_data = VEP_DATA,
        ref_fa = REF_FA,
    output:
        'work/{sample}/vep/somatic.AF_FILT.VEP.vcf',
    params:
        genome_build = ENSEMBL_GENOME,
        cache_version = VEP_ENSEMBL_RELEASE,
    resources:
        mem_mb=10000
    shell:
        'vep '
        '--input_file {input} '
        '--format vcf '
        '--output_file {output} '
        '--vcf '
        '--force_overwrite '
        '--symbol '
        '--terms SO '
        '--tsl '
        '--hgvs '
        '--fasta {input.ref_fa} '
        '--pick '
        '--plugin Downstream '
        '--plugin Wildtype '
        '--dir_plugins {input.vep_data}/Plugins '
        '--assembly {params.genome_build} '
        '--offline '
        '--cache '
        '--dir_cache {input.vep_data} '
        '--cache_version {params.cache_version}'

rule vcf_select:
    input:
        'work/{sample}/vep/somatic.AF_FILT.VEP.vcf'
    output:
        'work/{sample}/vep/somatic.AF_FILT.VEP.TUMOR_SAMPLE.vcf'
    params:
        sample = lambda wc, input: get_tumor_sample_name(input[0])
    shell:
        'bcftools view -s {params.sample} {input} | grep -v PEDIGREE > {output}'


#################
### Adding RNA information
bcbio_run_by_path = dict()
for sn, inp in inputs_by_sname.items():
    if inp['rna_bcbio'] not in bcbio_run_by_path:
        rna_run = BcbioProject(inp['rna_bcbio'], silent=True)
        assert rna_run.genome_build == 'hg38'
        bcbio_run_by_path[inp['rna_bcbio']] = rna_run


def get_rna_data(wc):
    rna_bcbio_path = inputs_by_sname[wc.sample]['rna_bcbio']
    rna_sname = inputs_by_sname[wc.sample]['rna_sample']

    rna_run = bcbio_run_by_path[rna_bcbio_path]
    rna_sample = [s for s in rna_run.samples if s.name == rna_sname][0]
    rna_bam = rna_sample.bam
    rna_counts_tx = join(rna_sample.dirpath, 'kallisto', 'abundance.tsv')
    rna_pizzly_prefix = join(rna_sample.dirpath, 'pizzly', rna_sname)
    fastq1, fastq2 = None, None
    fastq = [f for f in rna_sample.sample_info['files'] if not f.endswith('.bam') and verify_file(f, silent=True)]
    if fastq:
        if len(fastq) != 2:
            critical(f'Expected either 1 BAM file, or 2 FASTQ files in "files" field in bcbio config '
                     f'at {rna_bcbio_path}, sample {rna_sname}')
        fastq1 = fastq[0]
        fastq2 = fastq[1]
    else:
        bam = rna_sample.sample_info['files'][0]
        assert bam.endswith('bam'), f'Input for sample {rna_sname} is neither a pair of fastq nor a BAM file: {bam}'
        bam_name = splitext(basename(bam))[0]
        fastq1 = verify_file(join(rna_run.work_dir, 'align_prep', f'{bam_name}-1.fq.gz'))
        fastq2 = verify_file(join(rna_run.work_dir, 'align_prep', f'{bam_name}-2.fq.gz'))
        if not fastq1 and not fastq2:
            critical(f'Could not find RNAseq input fastq for requantification at {rna_bcbio_path}, sample {rna_sname}')

    return dict(
        bam=rna_bam,
        counts_tx=rna_counts_tx,
        pizzly_prefix=rna_pizzly_prefix,
        pizzly_tsv=rna_pizzly_prefix + '-flat-filtered.tsv',
        pizzly_json=rna_pizzly_prefix + '.json',
        pizzly_fasta=rna_pizzly_prefix + '.fusions.fasta',
        fastq1=fastq1,
        fastq2=fastq2,
    )

################
### Variant coverage and expression (for pVACseq coverageFilter)

rule extract_snps_sites:  # 1-based coordinates
    input:
        'work/{sample}/vep/somatic.AF_FILT.VEP.TUMOR_SAMPLE.vcf'
    output:
        'work/{sample}/somatic_snp_sites.txt',
    shell:
        'bcftools view -v snps {input} | '
        'bcftools query -f "%CHROM \\t %POS \\n" | '
        'awk \'{{ OFS="\\t" }} {{ print $1,$2,$2 }}\' > {output}'

rule extract_indels_sites:
    input:
        'work/{sample}/vep/somatic.AF_FILT.VEP.TUMOR_SAMPLE.vcf'
    output:
        'work/{sample}/somatic_indel_sites.txt',
    shell:
        'bcftools view -v indels {input} | '
        'bcftools query -f "%CHROM \\t %POS \\n" | '
        'awk \'{{ OFS="\\t" }}{{ print $1,$2,$2 }}\' > {output}'

rule rna_bam_readcount_snps:
    input:
        bam = lambda wc: get_rna_data(wc)['bam'],
        sites = 'work/{sample}/somatic_{type}_sites.txt',
        ref_fa = REF_FA,
    params:
        indel_opt = lambda wildcards: '-i ' if wildcards.type == 'indel' else ''
    output:
        'work/{sample}/trna_{type}_coverage_file.txt',
    resources:
        mem_mb=10000
    shell:
        'bam-readcount -f {input.ref_fa} --max-warnings 1 '
        '-l {input.sites} {input.bam} {params.indel_opt} > {output}'

# rule merge_rna_bam_readcount:
#     input:
#         expand(join(work_dir, 'trna_{type}_coverage_file.txt'), type=['indel', 'snp'])
#     output:
#         join(work_dir, 'trna_coverage_file.txt')
#     shell:
#         'cat {input} > {output}'

rule rna_coverage_anno_snp:
    input:
        vcf = 'work/{sample}/vep/somatic.AF_FILT.VEP.TUMOR_SAMPLE.vcf',
        cov_file = 'work/{sample}/trna_snp_coverage_file.txt',
    output:
        vcf = 'work/{sample}/afdp/somatic.AF_FILT.VEP.TUMOR_SAMPLE.RNAAFSNP.vcf'
    resources:
        mem_mb=10000
    shell:
        'vcf-readcount-annotator {input.vcf} {input.cov_file} '
        'RNA -t snv -o {output.vcf}'

rule rna_coverage_anno_indel:
    input:
        vcf = 'work/{sample}/afdp/somatic.AF_FILT.VEP.TUMOR_SAMPLE.RNAAFSNP.vcf',
        cov_file = 'work/{sample}/trna_indel_coverage_file.txt',
    output:
        vcf = 'work/{sample}/afdp/somatic.AF_FILT.VEP.TUMOR_SAMPLE.RNAAFSNP.RNAAFINDEL.vcf'
    shell:
        'vcf-readcount-annotator {input.vcf} {input.cov_file} '
        'RNA -t indel -o {output.vcf}'


rule transcript_expression_anno:
    input:
        vcf = 'work/{sample}/afdp/somatic.AF_FILT.VEP.TUMOR_SAMPLE.RNAAFSNP.RNAAFINDEL.vcf',
        expr_file = lambda wc: get_rna_data(wc)['counts_tx'],
    output:
        vcf = 'work/{sample}/afdp/somatic.AF_FILT.VEP.TUMOR_SAMPLE.RNAAFSNP.RNAAFINDEL.TX.vcf',
    shell:
        'vcf-expression-annotator {input.vcf} {input.expr_file} '
        'kallisto transcript -o {output.vcf}'

#################
### prepare fusions (for pVACfuse)

rule rna_pizzly_to_bedpe:
    input:
        lambda wc: get_rna_data(wc)['pizzly_tsv'],
        lambda wc: get_rna_data(wc)['pizzly_json'],
        lambda wc: get_rna_data(wc)['pizzly_fasta'],
        fastq1 = lambda wc: get_rna_data(wc)['fastq1'],
        fastq2 = lambda wc: get_rna_data(wc)['fastq2'],
    params:
        pizzly_prefix = lambda wc: get_rna_data(wc)['pizzly_prefix'],
        pep_5_prime_flank = PEP_5_PRIME_FLANK,
        pyensembl_basedir = dirname(PYENSEMBL_DIR),
        pyensembl_release = PIZZLY_ENSEMBL_RELEASE,
        pizzly_ref_fa_opt = f'--pizzly-ref-fa {PIZZLY_REF_FA} ' if PIZZLY_REF_FA else '',
    output:
        bedpe = 'work/{sample}/pizzly/{sample}.bedpe'
    resources:
        mem_mb=40000
    shell:
        'export PYENSEMBL_CACHE_DIR={params.pyensembl_basedir} && '
        'pizzly_to_bedpe.py {params.pizzly_prefix} -o {output.bedpe} -e {params.pyensembl_release} '
        '-p {params.pep_5_prime_flank} -r {input.fastq1} -r {input.fastq2} '
        '{params.pizzly_ref_fa_opt}'

rule run_pvacseq:
    input:
        vcf       = 'work/{sample}/afdp/somatic.AF_FILT.VEP.TUMOR_SAMPLE.RNAAFSNP.RNAAFINDEL.TX.vcf',
        hla_types = 'work/{sample}/hla_line.txt',
    output:
        expand('{{sample}}/pvacseq_results/MHC_Class_{CLS}/{{sample}}.filtered.condensed.ranked.tsv', CLS=['I', 'II']),
    params:
        out_dir = '{sample}/pvacseq_results'
    threads: 10
    resources:
        mem_mb=10000
    run:
        with open(input.vcf) as fh:
            lines_num = [l for l in fh.readlines() if not l.startswith('#')]
        if len(lines_num) == 0:
            warn('No mutations to run pVACseq.')
            shell('touch {output}')
        else:
            shell(_pvac_cmdl(
                'pvacseq',
                '{input.vcf}',
                wildcards.sample,
                '{input.hla_types}',
                '{params.out_dir}',
                threads,
                f'--trna-cov {MIN_RNA_COV} '
                f'--trna-vaf {MIN_RNA_VAF} '
                f'--expn-val {MIN_EXPN} '
            ))
            for of in output:
                if not isfile(of):
                    shell(f'touch {of}')

rule run_pvacfuse:
    input:
        bedpe     = 'work/{sample}/pizzly/{sample}.bedpe',
        hla_types = 'work/{sample}/hla_line.txt',
    output:
        expand('{{sample}}/pvacfuse_results/MHC_Class_{CLS}/{{sample}}.filtered.condensed.ranked.tsv', CLS=['I', 'II']),
    params:
        out_dir = '{sample}/pvacfuse_results',
    threads: 10
    resources:
        mem_mb=10000
    run:
        with open(input.bedpe) as fh:
            lines_num = [l for l in fh.readlines() if not l.startswith('chr')]
        if len(lines_num) == 0:
            warn('No fusions to run pVACfuse.')
            shell('touch {output}')
        else:
            bedpe = join(params.out_dir, wildcards.sample + '.bedpe')
            shell(f'grep -v ^chr {input.bedpe} > {bedpe}')
            shell(_pvac_cmdl(
                'pvacfuse',
                bedpe,
                wildcards.sample,
                '{input.hla_types}',
                '{params.out_dir}',
                threads,
            ))
            for of in output:
                if not isfile(of):
                    shell(f'touch {of}')


rule balachandran_epitopes_to_fasta:
    input:
        pvac_output = '{sample}/pvac{source}_results/MHC_Class_{cls}/{sample}.filtered.condensed.ranked.tsv',
    output:
        fasta = 'work/{sample}/quality_{source}/MHC_Class_{cls}/alignments/neoantigens_{sample}.fasta',
        list  = 'work/{sample}/quality_{source}/MHC_Class_{cls}/neoantigens.tsv',
    params:
        sname = lambda wc: wc.sample
    run:
        import pandas as pd
        epitopes = []
        try:
            data = pd.read_csv(input.pvac_output, sep='\t')
        except:
            pass
        else:
            epitopes.extend(zip(data['MT Epitope Seq'], data['Median WT Score'], data['Median MT Score']))

        with open(output.fasta, 'w') as fasta_out, open(output.list, 'w') as list_out:
            list_out = csv.DictWriter(list_out, dialect='excel-tab',
                fieldnames=['id', 'sample', 'epitope', 'wt_score', 'mt_score'])
            list_out.writeheader()
            for i, (peptide, wt_score, mt_score) in enumerate(epitopes):
                fasta_out.write(f'>M_{i}\n{peptide}\n')
                list_out.writerow(dict(
                    id=i,
                    sample=params.sname,
                    epitope=peptide,
                    wt_score=wt_score,
                    mt_score=mt_score,
                ))


rule balachandran_blast_epitopes:
    input:
        fasta = rules.balachandran_epitopes_to_fasta.output.fasta,
        iedb_fasta = join(balachandran.package_path(), 'data', 'iedb.fasta'),
    output:
        xml = 'work/{sample}/quality_{source}/MHC_Class_{cls}/alignments/neoantigens_{sample}_iedb.xml',
    resources:
        mem_mb=6000
    shell:
        'blastp -query {input.fasta} ' 
        '-db {input.iedb_fasta} '
        '-outfmt 5 -evalue 100000000 -gapopen 11 -gapextend 1 ' 
        '> {output.xml}'


rule balachandran_epitopes_quality:
    input:
        alignments = rules.balachandran_blast_epitopes.output.xml,
        script = join(balachandran.package_path(), 'main.py'),
        list = rules.balachandran_epitopes_to_fasta.output.list,
    output:
        txt = 'work/{sample}/quality_{source}/MHC_Class_{cls}/{sample}_neontigen_quality.tsv'
    params:
        alignments_dir = lambda wc, input, output: dirname(input.alignments),
    shell:
        'python {input.script} -l {input.list} -a {params.alignments_dir} '
        '-o {output.txt}'


rule summarize:
    input:
        expand('work/{sample}/quality_{{source}}/MHC_Class_{{cls}}/{sample}_neontigen_quality.tsv',
               sample=inputs_by_sname.keys())
    output:
        summary = 'MHC_Class_{cls}_summary_{source}.tsv'
    run:
        nag_summarize(inputs_by_sname.keys(), input, output.summary)
