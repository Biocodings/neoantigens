import glob
import os
from os.path import join, abspath, dirname, isfile, basename, splitext
import csv

from ngs_utils.vcf_utils import get_tumor_sample_name, vcf_contains_field, iter_vcf
from ngs_utils.bcbio import BcbioProject
from ngs_utils.logger import critical, info, debug, warn, err
from ngs_utils.file_utils import verify_file, verify_dir, open_gzipsafe
from reference_data import api as refdata
from nag import balachandran
from nag import nag_summarize

shell.executable(os.environ.get('SHELL', 'bash'))
shell.prefix("")


if config.get('genomes_dir'):
    refdata.find_genomes_dir(config.get('genomes_dir'))
print('Genomes dir:', refdata.genomes_dir)


inputs_by_sname = dict()
if config.get('input_tsv'):
    include_samples = None
    if 'include' in config:
        include_samples = config['include'].split(',')
    exclude_samples = None
    if 'exclude' in config:
        exclude_samples = config['exclude'].split(',')

    with open(config['input_tsv']) as f:
        csv_reader = csv.DictReader(f, delimiter='\t')
        for e in csv_reader:
            if include_samples and e['sample'] not in include_samples:
                print(f"excluding {e['sample']} because not in {include_samples}")
                continue
            if exclude_samples and e['sample'] in exclude_samples:
                print(f"excluding {e['sample']} because in {exclude_samples}")
                continue
            print(f"using {e['sample']}")
            inputs_by_sname[e['sample']] = e
else:
    inputs_by_sname[config['sample']] = dict(
        sample=config['sample'],
        somatic_vcf=config['somatic_vcf'],
        rna_bcbio=config['rna_bcbio'],
        rna_sample=config['rna_sample'],
        optitype_file=config['optitype_file']
    )
for sn, inp in inputs_by_sname.items():
    inp['rna_bcbio'] = verify_dir(inp['rna_bcbio'], is_critical=True, description=f'rna_bcbio for {sn}')
    inp['optitype_file'] = verify_file(inp['optitype_file'], is_critical=True, description=f'optitype_file for {sn}')
    inp['somatic_vcf'] = verify_file(inp['somatic_vcf'], is_critical=True, description=f'somatic_vcf file for {sn}')

GENOME                 = 'hg38'
ENSEMBL_GENOME         = 'GRCh38'
PIZZLY_ENSEMBL_RELEASE = config.get('ensembl_release', 95)
VEP_ENSEMBL_RELEASE    = config.get('ensembl_release', 98)
PYENSEMBL_DIR          = config.get('ensembl_dir', refdata.get_ref_file(genome=GENOME, key='pyensembl_data'))
VEP_DATA               = glob.glob(join(refdata.get_ref_file(genome=GENOME, key='pcgr_data'), '*/.vep'))[0]
IEDB_DIR               = config.get('iedb_dir', refdata.get_ref_file(key='iedb_dir'))
REF_FA                 = config.get('ref_fa', refdata.get_ref_file(genome=GENOME, key='fa'))
USE_REMOTE_DB          = config.get('internet_is_on', False)


# pVACtools parameters
EPITOPE_LENGTHS = '9'  #'8,9,10,11'
PREDICTORS_MHCI = [
    'NetMHCcons',  # NetMHCcons is ensemble of NetMHC,NetMHCpan,PickPocket
    'MHCflurry',
    'MHCnuggetsI',
    'SMM',
    'SMMPMBEC',
]
PREDICTORS_MHCII = [
    'NetMHCIIpan',
    'MHCnuggetsII',
    'NNalign',
    'SMMalign'
]

# pVACfuse parameters
PEP_5_PRIME_FLANK = 14  # Aminoacids to pick from both sides of the fusion junction peptide.
    # It should be long enough so your longest desigred epitope just covers the junction. Say,
    # if the longest epitope length is 11, use PEP_5_PRIME_FLANK of 10. Since MHC_II algorithms
    # always call epitopes of the length 15, we set PEP_5_PRIME_FLANK to 14.

# pVACseq parameters
MIN_NORMAL_DP = 5
MAX_NORMAL_AF = 0.02
MIN_TUMOR_DP = MIN_RNA_DP = 10
MIN_TUMOR_AF = MIN_RNA_AF = config.get('min_tumor_af', 0.1)
MIN_EXPN = 1.0  # TPM expression should be at least 1
EXCLUDE_NA = False


stages = config.get('stages', ['seq', 'fuse', 'qual'])


rule all:
    input:
        expand('MHC_Class_{cls}_summary_{source}.tsv', cls=['I', 'II'],
               source=[s for s in ['seq', 'fuse'] if s in stages]),
        expand('{sample}/pvacseq_results/MHC_Class_{cls}/{sample}.filtered.condensed.ranked.tsv',
               cls=['I', 'II'], sample=inputs_by_sname.keys()) \
             if ('qual' in stages and 'seq' in stages) else [],
        expand('{sample}/pvacfuse_results/MHC_Class_{cls}/{sample}.filtered.condensed.ranked.tsv',
               cls=['I', 'II'], sample=inputs_by_sname.keys()) \
             if ('qual' in stages and 'fuse' in stages) else [],


def _pvac_cmdl(tool, input, sample, hla_types, output_dir, threads, other_params=''):
    cmd = (
        f'{tool} run '
        f'{input} '
        f'{sample} '
        f'"$(cat {hla_types})" '
        f'{" ".join(PREDICTORS_MHCI + PREDICTORS_MHCII)} '
        f'{output_dir} '
        f'-e {EPITOPE_LENGTHS} '
        f'--top-score-metric=lowest '
        f'--n-threads {threads} '
        f'--iedb-install-directory {IEDB_DIR} '
        f'--keep-tmp-files '
        f'{other_params} '
    )
    if USE_REMOTE_DB:
        cmd += \
            f' --net-chop-method cterm ' \
            f'--netmhc-stab'
    if EXCLUDE_NA:
        cmd += f' --exclude-NAs'
    return cmd


#################
### Loading RNA information
bcbio_run_by_path = dict()
for sn, inp in inputs_by_sname.items():
    if inp['rna_bcbio'] not in bcbio_run_by_path:
        rna_run = BcbioProject(inp['rna_bcbio'], silent=True)
        assert rna_run.genome_build == 'hg38'
        bcbio_run_by_path[inp['rna_bcbio']] = rna_run

def get_rna_data(wc):
    rna_bcbio_path = inputs_by_sname[wc.sample]['rna_bcbio']
    rna_sname = inputs_by_sname[wc.sample]['rna_sample']

    rna_run = bcbio_run_by_path[rna_bcbio_path]
    rna_sample = [s for s in rna_run.samples if s.name == rna_sname][0]
    rna_bam = rna_sample.bam
    rna_counts_tx = join(rna_sample.dirpath, 'kallisto', 'abundance.tsv')
    rna_pizzly_prefix = join(rna_sample.dirpath, 'pizzly', rna_sname)

    fastq = [f for f in rna_sample.sample_info['files'] if not f.endswith('.bam') and verify_file(f, silent=True)]
    if fastq:
        if len(fastq) != 2:
            critical(f'Expected either 1 BAM file, or 2 FASTQ files in "files" field in bcbio config '
                     f'at {rna_bcbio_path}, sample {rna_sname}')
        fastq1 = fastq[0]
        fastq2 = fastq[1]
        info('Found FASTQ files in input config `files`')
    else:
        bam = rna_sample.sample_info['files'][0]
        assert bam.endswith('bam'), f'Input for sample {rna_sname} is neither a pair of fastq nor a BAM file: {bam}'
        bam_name = splitext(basename(bam))[0]
        fastq1 = verify_file(join(rna_run.work_dir, 'align_prep', f'{bam_name}-1.fq.gz'))
        fastq2 = verify_file(join(rna_run.work_dir, 'align_prep', f'{bam_name}-2.fq.gz'))
        if not fastq1 and not fastq2:
            warn(f'Could not find RNAseq input fastq for requantification at '
                 f'{rna_bcbio_path}, sample {rna_sname}')
        info('Found FASTQ files in the `work/align_prep` directory')

    res = dict(
        bam=rna_bam,
        counts_tx=rna_counts_tx,
        pizzly_prefix=rna_pizzly_prefix,
        pizzly_tsv=rna_pizzly_prefix + '-flat-filtered.tsv',
        pizzly_json=rna_pizzly_prefix + '.json',
        pizzly_fasta=rna_pizzly_prefix + '.fusions.fasta',
    )
    if fastq1:
        res['fastq1'] = fastq1
    if fastq2:
        res['fastq2'] = fastq2
    return res


################
### HLA types

# t_optitype = join(batch.tumor.dirpath, batch.tumor.name + '-hla-optitype.csv')

rule prep_hla:
    """ Create a file containing a single line of comma-separated HLA alleles
    """
    input:
         lambda wc: inputs_by_sname[wc.sample]['optitype_file']
    output:
        'work/{sample}/hla_line.txt'
    run:
        alleles = []
        with open(input[0]) as f:
            for i, l in enumerate(f):
                if i != 0:
                    fs = l.strip().split('\t')[1:7]
                    alleles.extend([f'HLA-{f}' for f in fs])
        with open(output[0], 'w') as f:
            f.write(','.join(alleles))


################
### Small mutations (for pVACseq)

rule drop_older_csq:
    input:
        vcf = lambda wc: inputs_by_sname[wc.sample]['somatic_vcf'],
    output:
        vcf = 'work/{sample}/step1_vcf_drop_older_csq/somatic.vcf.gz',
    run:
        cmd = f'cat {input.vcf}'
        # remove previous VEP annotation:
        filts_to_remove = [f'{f}' for f in ['INFO/CSQ'] if vcf_contains_field(input.vcf, f)]
        if filts_to_remove:
            cmd += f' | bcftools annotate -x "' + ','.join(f'{f}' for f in filts_to_remove) + '"'
        cmd += ' | bcftools view -f.,PASS -Oz -o {output.vcf} && tabix -p vcf {output.vcf}'
        shell(cmd)

# The --symbol option will include gene symbol in the annotation.
# The --terms SO option will result in Sequence Ontology terms being used for the consequences.
# The --tsl option adds transcript support level information to the annotation.
# The --hgvs option will result in HGVS identifiers being added to the annotation.
#   Requires the usage of the --fasta argument.
# The --cache option will result in the VEP cache being used for annotation.
# The --plugin Downstream option will run the Downstream plugin which will compute the
#   downstream protein sequence after a frameshift.
# The --plugin Wildtype option will run the Wildtype plugin which will include the transcript
#   protein sequence in the annotation.
rule vcf_vep:
    input:
        vcf = 'work/{sample}/step1_vcf_drop_older_csq/somatic.vcf.gz',
        vep_data = VEP_DATA,
        ref_fa = REF_FA,
    output:
        vcf = 'work/{sample}/step2_vcf_vep/somatic.vcf.gz',
    params:
        genome_build = ENSEMBL_GENOME,
        cache_version = VEP_ENSEMBL_RELEASE,
    resources:
        mem_mb=10000
    shell:
        'vep '
        '--input_file {input} '
        '--format vcf '
        '--output_file {output} '
        '--vcf '
        '--force_overwrite '
        '--symbol '
        '--terms SO '
        '--tsl '
        '--hgvs '
        '--fasta {input.ref_fa} '
        '--pick '
        '--plugin Downstream '
        '--plugin Wildtype '
        '--dir_plugins {input.vep_data}/Plugins '
        '--assembly {params.genome_build} '
        '--offline '
        '--cache '
        '--dir_cache {input.vep_data} '
        '--cache_version {params.cache_version}'

rule transcript_expression_anno:
    input:
        vcf = 'work/{sample}/step2_vcf_vep/somatic.vcf.gz',
        expr_file = lambda wc: get_rna_data(wc)['counts_tx'],
    output:
        vcf = 'work/{sample}/step3_vcf_expr_anno/somatic.vcf.gz',
    run:
        import pandas as pd
        df = pd.read_csv(input.expr_file, sep='\t', index_col='target_id')
        def _proc_rec(rec, vcf):
            tpm_by_enst = dict()
            csq = rec.INFO['CSQ']
            if isinstance(csq, str):
                csq = csq.split(',')
            for annotation in csq:
                enst = annotation.split(',')[0].split('|')[6]
                try:
                    tpm = df.loc[enst, 'tpm']
                except:
                    pass
                else:
                    tpm_by_enst[enst] = tpm
            if tpm_by_enst:
                val = [f'{enst}|{tpm}' for enst, tpm in tpm_by_enst.items()]
                rec.INFO['TranscriptExprTPM'] = ','.join(val)
                rec.INFO['MaxTranscriptExprTPM'] = max(tpm_by_enst.values())
            return rec

        def _proc_hdr(vcf):
            vcf.add_info_to_header({'ID': 'TranscriptExprTPM',
                'Description': 'TPM value by transcript, reported by Kallisto in abundance.tsv',
                'Type': 'String', 'Number': '.'})
            vcf.add_info_to_header({'ID': 'MaxTranscriptExprTPM',
                'Description': 'Max TPM value across all transcripts, '
                               'reported by Kallisto in abundance.tsv',
                'Type': 'Float', 'Number': '1'})

        iter_vcf(input.vcf, output.vcf, proc_rec=_proc_rec, proc_hdr=_proc_hdr)

rule vcf_filter:  # {batch}
    input:
        vcf = 'work/{sample}/step3_vcf_expr_anno/somatic.vcf.gz',
    output:
        vcf = 'work/{sample}/step4_vcf_filter/somatic.vcf.gz',
    shell:
        'bcftools filter {input.vcf} -i "'
        'TUMOR_AF>={MIN_TUMOR_AF} & '
        'TUMOR_DP>={MIN_TUMOR_DP} & '
        'RNA_AF>={MIN_RNA_AF} & '
        'RNA_DP>={MIN_RNA_DP} & '
        'MaxTranscriptExprTPM>={MIN_EXPN} & '
        'NORMAL_AF<={MAX_NORMAL_AF} & '
        'NORMAL_DP>={MIN_NORMAL_DP}" '
        '-Oz -o {output.vcf} && tabix {output.vcf}'


#################
### prepare fusions (for pVACfuse)

rule rna_pizzly_to_bedpe:
    input:
        lambda wc: get_rna_data(wc)['pizzly_tsv'],
        lambda wc: get_rna_data(wc)['pizzly_json'],
        lambda wc: get_rna_data(wc)['pizzly_fasta'],
        pizzly_ref_fa = refdata.get_ref_file(genome='hg38', key='pizzly_ref_fa'),
    params:
        pizzly_prefix = lambda wc: get_rna_data(wc)['pizzly_prefix'],
        pep_5_prime_flank = PEP_5_PRIME_FLANK,
        pyensembl_basedir = dirname(PYENSEMBL_DIR),
        pyensembl_release = PIZZLY_ENSEMBL_RELEASE,
    output:
        bedpe = 'work/{sample}/pizzly/{sample}.bedpe'
    resources:
        mem_mb=40000
    run:
        fastq1 = get_rna_data(wildcards).get('fastq1')
        fastq2 = get_rna_data(wildcards).get('fastq2')
        fastq_opt = ''
        if fastq1 and fastq2:
            fastq_opt = f' -r {fastq1} -r {fastq2}'
        shell(
            'export PYENSEMBL_CACHE_DIR={params.pyensembl_basedir} && '
            'pizzly_to_bedpe.py {params.pizzly_prefix} -o {output.bedpe} -e {params.pyensembl_release} '
            '-p {params.pep_5_prime_flank} {fastq_opt} --pizzly-ref-fa {input.pizzly_ref_fa}'
        )

rule run_pvacseq:
    input:
        vcf = 'work/{sample}/step4_vcf_filter/somatic.vcf.gz',
        hla_types = 'work/{sample}/hla_line.txt',
    output:
        fpaths = expand(
            '{{sample}}/pvacseq_results/MHC_Class_{CLS}/{{sample}}.filtered.condensed.ranked.tsv',
            CLS=['I', 'II']),
        out_dir = directory('{sample}/pvacseq_results')
    threads: 10
    resources:
        mem_mb=10000
    run:
        with open_gzipsafe(input.vcf) as fh:
            lines_num = [l for l in fh.readlines() if not l.startswith('#')]
        if len(lines_num) == 0:
            warn('No mutations to run pVACseq.')
            shell('touch {output.fpaths}')
        else:
            shell(_pvac_cmdl(
                'pvacseq',
                f'{input.vcf}',
                wildcards.sample,
                f'{input.hla_types}',
                f'{output.out_dir}',
                threads
            ))
            shell(f'touch {output.fpaths}')

rule run_pvacfuse:
    input:
        bedpe     = 'work/{sample}/pizzly/{sample}.bedpe',
        hla_types = 'work/{sample}/hla_line.txt',
    output:
        fpaths = expand(
            '{{sample}}/pvacfuse_results/MHC_Class_{CLS}/{{sample}}.filtered.condensed.ranked.tsv',
            CLS=['I', 'II']),
        out_dir = directory('{sample}/pvacfuse_results'),
    threads: 10
    resources:
        mem_mb=10000
    run:
        with open(input.bedpe) as fh:
            lines_num = [l for l in fh.readlines() if not l.startswith('chr')]
        if len(lines_num) == 0:
            warn('No fusions to run pVACfuse.')
            shell('touch {output.fpaths}')
        else:
            bedpe = join(output.out_dir, wildcards.sample + '.bedpe')
            shell(f'grep -v ^chr {input.bedpe} > {bedpe}')
            shell(_pvac_cmdl(
                'pvacfuse',
                bedpe,
                wildcards.sample,
                f'{input.hla_types}',
                f'{output.out_dir}',
                threads,
            ))
            shell(f'touch {output.fpaths}')


rule balachandran_epitopes_to_fasta:
    input:
        pvac_output = '{sample}/pvac{source}_results/MHC_Class_{cls}/{sample}.filtered.condensed.ranked.tsv',
    output:
        fasta = 'work/{sample}/quality_{source}/MHC_Class_{cls}/alignments/neoantigens_{sample}.fasta',
        list  = 'work/{sample}/quality_{source}/MHC_Class_{cls}/neoantigens.tsv',
    params:
        sname = lambda wc: wc.sample
    run:
        import pandas as pd
        epitopes = []
        try:
            data = pd.read_csv(input.pvac_output, sep='\t')
        except:
            pass
        else:
            epitopes.extend(zip(data['MT Epitope Seq'], data['Median WT Score'], data['Median MT Score']))

        with open(output.fasta, 'w') as fasta_out, open(output.list, 'w') as list_out:
            list_out = csv.DictWriter(list_out, dialect='excel-tab',
                fieldnames=['id', 'sample', 'epitope', 'wt_score', 'mt_score'])
            list_out.writeheader()
            for i, (peptide, wt_score, mt_score) in enumerate(epitopes):
                fasta_out.write(f'>M_{i}\n{peptide}\n')
                list_out.writerow(dict(
                    id=i,
                    sample=params.sname,
                    epitope=peptide,
                    wt_score=wt_score,
                    mt_score=mt_score,
                ))


rule balachandran_blast_epitopes:
    input:
        fasta = rules.balachandran_epitopes_to_fasta.output.fasta,
        iedb_fasta = join(balachandran.package_path(), 'data', 'iedb.fasta'),
    output:
        xml = 'work/{sample}/quality_{source}/MHC_Class_{cls}/alignments/neoantigens_{sample}_iedb.xml',
    resources:
        mem_mb=6000
    shell:
        'blastp -query {input.fasta} ' 
        '-db {input.iedb_fasta} '
        '-outfmt 5 -evalue 100000000 -gapopen 11 -gapextend 1 ' 
        '> {output.xml}'


rule balachandran_epitopes_quality:
    input:
        alignments = rules.balachandran_blast_epitopes.output.xml,
        script = join(balachandran.package_path(), 'main.py'),
        list = rules.balachandran_epitopes_to_fasta.output.list,
    output:
        txt = 'work/{sample}/quality_{source}/MHC_Class_{cls}/{sample}_neontigen_quality.tsv'
    params:
        alignments_dir = lambda wc, input, output: dirname(input.alignments),
    shell:
        'python {input.script} -l {input.list} -a {params.alignments_dir} '
        '-o {output.txt}'


rule summarize:
    input:
        expand('work/{sample}/quality_{{source}}/MHC_Class_{{cls}}/{sample}_neontigen_quality.tsv',
               sample=inputs_by_sname.keys())
    output:
        summary = 'MHC_Class_{cls}_summary_{source}.tsv'
    run:
        nag_summarize(inputs_by_sname.keys(), input, output.summary)
